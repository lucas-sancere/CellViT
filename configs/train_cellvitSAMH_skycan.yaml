
logging:
  mode: offline                         # "online" or "offline" [str]
  project: celllVIT-skycan              # Name of project to use [str]
  notes: CellViT-SAM-H                    # Notes about the run, verbose description [str]
  log_comment: CellViT-SAM-H-1            # Comment to add to name the local logging folder [str]
  tags:                                 # List of tags, e.g., ["baseline", "run1"] [str]
    - train1
    # - "tag2"
    # - "..."
  wandb_dir: /data/lsancere/Ada_Codes/CellViT/results/skycanucleus/               # Direcotry to store the wandb file. CAREFUL: Directory must exists [str]
  log_dir: /data/lsancere/Ada_Codes/CellViT/results/skycanucleus/logs/            # Direcotry to store all logging related files and outputs [str]
  level: Debug                                                                    # Level of logging must be either ["critical", "error", "warning", "info", "debug"] [str]
 # log_images:              # If images should be logged to WandB for this run. [bool] [Optional, defaults to False]
 #  group:                  # WandB group tag [str] [Optional, defaults to None]

# seeding
random_seed: 19             # Seed for numpy, pytorch etc. [int]

# hardware
gpu: 2                     # Number of GPU to run experiment on [int]

# setting paths and dataset
data:
  dataset: skycanucleus      # Name of dataset, currently supported: PanNuke, Conic. Select your dataset and the script will autoamtically select the corresponding experiment [str]
  dataset_path: /data/lsancere/Ada_Codes/CellViT/configs/datasets/SkycaNucleus/                          # Path to dataset, compare ./docs/readmes/pannuke.md for further details [str]
  train_folds: 0             # List of fold Numbers to use for training [list[int]]
#  val_split:                # Percentage of training set that should be used for validation. Either val_split or val_fold must be provided, not both. [float]
  val_folds: 1               # List of fold Numbers to use for validation [list[int]]
  test_folds: 1              # List of fold Numbers to use for final testing [list[int]]
  num_nuclei_classes: 6      # Number of different nuclei classes (including background!, e.g. 5 nuclei classes + background = 6) [int]
  num_tissue_classes: 1
  input_shape: 256           # Input shape of data. [int] [Optional, defaults to 256]

# model options
model:
  backbone: SAM-H                                                                                   # Backbone Type: Options are: default, ViT256, SAM-B, SAM-L, SAM-H
  pretrained_encoder: /data/lsancere/Ada_Codes/CellViT/models/pretrained/CellViT-SAM-H-x40.pth       # Set path to a pretrained encoder [str]
  #pretrained: None                                                                                  # Path to a pretrained model (.pt file) [str, default None]
  embed_dim: 384                                                                                     # Embedding dimension for ViT - typical values are 384 (ViT-S), 768 (ViT-B), 1024 (ViT-L), 1280 (ViT-H) [int]
  input_channels: 3                                                                                  # Number of input channels, usually 3 for RGB [int, default 3]
  depth: 12                                                                                          # Number of Transformer Blocks to use - typical values are 12 (ViT-S), 12 (ViT-B), 24 (ViT-L), 32 (ViT-H) [int]
  num_heads: 6                                                                                       # Number of attention heads for MHA - typical values are 6 (ViT-S), 12 (ViT-B), 16 (ViT-L), 16 (ViT-H) [int]
  #extract_layers:                                                                                   # List of layers to extract for skip connections - starting from 1 with a maximum value equals the depth [int]
  shared_decoders: false                                                                             # If decoder networks should be shared except for the heads. [bool] [Optional, defaults to False]
  #regression_loss:                                                                                  # If regression loss should be used for binary prediction head. [bool] [Optional, defaults to False]

# loss function settings (best shown by an example). See all implemented loss functions in base_ml.base_loss module
loss:
  nuclei_binary_map:
    focaltverskyloss:
      loss_fn: FocalTverskyLoss
      weight: 1
    dice:
      loss_fn: dice_loss
      weight: 1
  hv_map:
    mse:
      loss_fn: mse_loss_maps
      weight: 2.5
    msge:
      loss_fn: msge_loss_maps
      weight: 8
  nuclei_type_map:
    bce:
      loss_fn: xentropy_loss
      weight: 0.5
    dice:
      loss_fn: dice_loss
      weight: 0.2
    mcfocaltverskyloss:
      loss_fn: MCFocalTverskyLoss
      weight: 0.5
      args:
        num_classes: 6
  tissue_types:
    ce:
      loss_fn: CrossEntropyLoss
      weight: 0.1


# training options
training:
  batch_size: 32            # Training Batch size [int]
  epochs: 200               # Number of Training Epochs to use [int]
  unfreeze_epoch: 25        # Epoch Number to unfreeze backbone [int]
  drop_rate: 0              # Dropout rate [float] [Optional, defaults to 0]
  attn_drop_rate: 0         # Dropout rate in attention layer [float] [Optional, defaults to 0]
  drop_path_rate: 0         # Dropout rate in paths [float] [Optional, defaults to 0]
  optimizer: AdamW          # Pytorch Optimizer Name. All pytorch optimizers (v1.13) are supported. [str]
  
  optimizer_hyperparameter:   # Hyperparamaters for the optimizers, must be named exactly as in the pytorch documation given for the selected optimizer
    lr: 0.001                 # e.g. learning-rate for Adam
    betas: [0.85, 0.9]        # e.g. betas for Adam
  
  early_stopping_patience: 200          # Number of epochs before applying early stopping after metric has not been improved. Metric used is total loss. [int]
    
  scheduler:                            # Learning rate scheduler. If no scheduler is selected here, then the learning rate stays constant
    scheduler_type: exponential         # Name of learning rate scheduler. Currently implemented: "constant", "exponential", "cosine". [str]
    hyperparameters:
      gamma: 0.95                       # gamma [default 0.95] for "exponential", "eta_min" [default 1e-5] for CosineAnnealingLR
  
  sampling_strategy: random             # Sampling strategy. Implemented are "random", "cell", "tissue" and "cell+tissue" [str] [Optional, defaults to "random"]
  sampling_gamma: 0.85                  # Gamma for balancing sampling. Must be between 0 (equal weights) and 1 (100% oversampling) [float] [Optional, defaults to 1]
  mixed_precision: False                # Mixed precision Flag. [bool] [Optional, default False]
  eval_every: 1                         # Number of training epochs between every validation. If 1, alternating training and validation as commonly used. [int] [Optional, default 1]


# transformations, here all options are given. Remove transformations by removing them from this section
transformations:
  randomrotate90:              # RandomRotation90
    p: 0.5                     # Probability [float, between 0 and 1]
  horizontalflip:              # HorizontalFlip
    p: 0.5                     # Probability [float, between 0 and 1]
  verticalflip:                # VerticalFlip
    p: 0.5                     # Probability [float, between 0 and 1]
  downscale:                   # Downscaling
    p: 0.15                    # Probability [float, between 0 and 1]
    scale: 0.5                 # Scaling factor, maximum should be 0.5. Must be smaller than 1 [float, between 0 and 1]
  blur:                        # Blur
    p: 0.2                     # Probability [float, between 0 and 1]
    blur_limit: 10             # Bluring limit, maximum should be 10, recommended 10 [float]
  gaussnoise:                  # GaussianNoise
    p: 0.25                    # Probability [float, between 0 and 1]
    var_limit: 50              # Variance limit, maxmimum should be 50, recommended 10 [float]
  colorjitter:                 # ColorJitter
    p: 0.2                     # Probability [float, between 0 and 1]
    scale_setting: 0.25        # Scaling for contrast and brightness, recommended 0.25 [float]
    scale_color: 0.1           # Scaling for hue and saturation, recommended 0.1 [float]
  superpixels:                 # SuperPixels
    p: 0.1                     # Probability [float, between 0 and 1]
  zoomblur:                    # ZoomBlur
    p: 0.1                     # Probability [float, between 0 and 1]
  randomsizedcrop:             # RandomResizeCrop
    p: 0.2                     # Probability [float, between 0 and 1]
  elastictransform:            # ElasticTransform
    p: 0.2                     # Probability [float, between 0 and 1]
  normalize:                   # Normalization
    mean:
    - 0.5
    - 0.5
    - 0.5
    std:
    - 0.5
    - 0.5
    - 0.5                  

eval_checkpoint: best_checkpoint.pth            # Either select "best_checkpoint.pth", "latest_checkpoint.pth" or one of the intermediate checkpoint names, e.g., "checkpoint_100.pth"

dataset_config:
  tissue_types: 1
  nuclei_types: 6